#!/usr/bin/env python3
"""
ArXivçˆ¬å–é—®é¢˜è¯Šæ–­å·¥å…·
ç”¨äºè°ƒè¯•ä¸ºä»€ä¹ˆè·å–çš„æ–‡ç« æ•°é‡å°‘äºæœŸæœ›å€¼
"""

import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlencode

def test_arxiv_api(search_query, max_results=100):
    """
    ç›´æ¥æµ‹è¯•ArXiv APIå“åº”
    """
    base_url = "http://export.arxiv.org/api/query"
    
    print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {search_query}")
    print(f"ğŸ“Š æœŸæœ›æœ€å¤§ç»“æœæ•°: {max_results}")
    print("-" * 60)
    
    # æµ‹è¯•ä¸åŒçš„æ‰¹æ¬¡å¤§å°
    batch_sizes = [50, 100, 200]
    
    for batch_size in batch_sizes:
        current_batch = min(batch_size, max_results)
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': current_batch,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        print(f"\nğŸ“¦ æµ‹è¯•æ‰¹æ¬¡å¤§å°: {current_batch}")
        print(f"ğŸŒ è¯·æ±‚URL: {base_url}?{urlencode(params)}")
        
        try:
            response = requests.get(base_url, params=params)
            response.raise_for_status()
            
            # è§£æXML
            root = ET.fromstring(response.text)
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'opensearch': 'http://a9.com/-/spec/opensearch/1.1/',
            }
            
            # è·å–æ€»ç»“æœæ•°
            total_results = root.find('opensearch:totalResults', namespaces)
            start_index = root.find('opensearch:startIndex', namespaces)
            items_per_page = root.find('opensearch:itemsPerPage', namespaces)
            
            # è®¡ç®—å®é™…æ¡ç›®æ•°
            entries = root.findall('atom:entry', namespaces)
            
            print(f"ğŸ“ˆ ArXivæŠ¥å‘Šçš„æ€»ç»“æœæ•°: {total_results.text if total_results is not None else 'Unknown'}")
            print(f"ğŸ“Œ èµ·å§‹ç´¢å¼•: {start_index.text if start_index is not None else 'Unknown'}")
            print(f"ğŸ“„ æ¯é¡µæ¡ç›®æ•°: {items_per_page.text if items_per_page is not None else 'Unknown'}")
            print(f"âœ… å®é™…è¿”å›çš„æ¡ç›®æ•°: {len(entries)}")
            
            # æ˜¾ç¤ºå‰å‡ ç¯‡æ–‡ç« çš„ä¿¡æ¯
            print(f"\nğŸ“‹ å‰3ç¯‡æ–‡ç« é¢„è§ˆ:")
            for i, entry in enumerate(entries[:3]):
                title_elem = entry.find('atom:title', namespaces)
                id_elem = entry.find('atom:id', namespaces)
                
                title = title_elem.text.strip() if title_elem is not None else "No title"
                arxiv_id = id_elem.text.split('/')[-1] if id_elem is not None else "No ID"
                
                print(f"  {i+1}. [{arxiv_id}] {title[:60]}...")
            
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        
        print("-" * 40)

def analyze_search_query(search_query):
    """
    åˆ†ææœç´¢æŸ¥è¯¢çš„æœ‰æ•ˆæ€§
    """
    print(f"\nğŸ” æœç´¢æŸ¥è¯¢åˆ†æ:")
    print(f"åŸå§‹æŸ¥è¯¢: {search_query}")
    
    # æ£€æŸ¥æŸ¥è¯¢çš„ç»„æˆéƒ¨åˆ†
    parts = search_query.split(' AND ')
    print(f"æŸ¥è¯¢ç»„ä»¶æ•°: {len(parts)}")
    
    for i, part in enumerate(parts, 1):
        print(f"  ç»„ä»¶ {i}: {part}")
        
        # åˆ†ææ¯ä¸ªç»„ä»¶
        if 'abs:' in part:
            keyword = part.replace('abs:', '').strip('"')
            print(f"    â†’ æ‘˜è¦å…³é”®è¯: '{keyword}'")
        elif 'submittedDate:' in part:
            date_range = part.replace('submittedDate:', '')
            print(f"    â†’ æäº¤æ—¥æœŸèŒƒå›´: {date_range}")
        elif 'cat:' in part:
            category = part.replace('cat:', '')
            print(f"    â†’ åˆ†ç±»: {category}")

def suggest_alternatives(search_query, actual_results, expected_results):
    """
    å»ºè®®æ›¿ä»£æœç´¢ç­–ç•¥
    """
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    if actual_results < expected_results:
        print(f"ğŸ“‰ è·å¾— {actual_results} ç¯‡ï¼ŒæœŸæœ› {expected_results} ç¯‡")
        print(f"\nğŸ”§ å¯èƒ½çš„ä¼˜åŒ–ç­–ç•¥:")
        
        # å»ºè®®æ”¾å®½æœç´¢æ¡ä»¶
        if ' AND ' in search_query:
            print("1. ä½¿ç”¨ OR æ›¿ä»£éƒ¨åˆ† AND æ¡ä»¶ï¼Œæ‰©å¤§æœç´¢èŒƒå›´")
            alternative = search_query.replace(' AND abs:', ' OR abs:')
            print(f"   ç¤ºä¾‹: {alternative}")
        
        # å»ºè®®ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
        if '"' in search_query:
            print("2. ç§»é™¤å¼•å·ï¼Œä½¿ç”¨æ›´å®½æ³›çš„å…³é”®è¯åŒ¹é…")
            alternative = search_query.replace('"', '')
            print(f"   ç¤ºä¾‹: {alternative}")
        
        # å»ºè®®æ‰©å¤§æ—¶é—´èŒƒå›´
        if 'submittedDate:' in search_query:
            print("3. æ‰©å¤§æ—¶é—´èŒƒå›´")
            print("   ç¤ºä¾‹: å°†å¼€å§‹æ—¥æœŸæå‰1å¹´")
        
        # å»ºè®®ä½¿ç”¨titleæœç´¢
        if 'abs:' in search_query and 'ti:' not in search_query:
            print("4. æ·»åŠ æ ‡é¢˜æœç´¢")
            title_query = search_query.replace('abs:', 'ti:')
            combined = f"({search_query}) OR ({title_query})"
            print(f"   ç¤ºä¾‹: {combined}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ” ArXivçˆ¬å–é—®é¢˜è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # ä½¿ç”¨æ‚¨çš„å®é™…æŸ¥è¯¢å‚æ•°
    search_query = 'abs:"tool use" AND abs:cost AND submittedDate:[20240101 TO 20250805]'
    max_results = 100
    
    # åˆ†ææŸ¥è¯¢
    analyze_search_query(search_query)
    
    # æµ‹è¯•APIå“åº”
    test_arxiv_api(search_query, max_results)
    
    # æä¾›å»ºè®®
    suggest_alternatives(search_query, 25, max_results)  # åŸºäºæ‚¨çš„å®é™…ç»“æœ
    
    print(f"\n" + "=" * 60)
    print("ğŸ¯ æ€»ç»“:")
    print("ArXiv APIæœ‰æ—¶ä¼šè¿”å›å°‘äºè¯·æ±‚æ•°é‡çš„ç»“æœï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚")
    print("è¿™é€šå¸¸è¡¨ç¤ºæ•°æ®åº“ä¸­åŒ¹é…çš„æ–‡ç« ç¡®å®æœ‰é™ã€‚")
    print("å°è¯•è°ƒæ•´æœç´¢ç­–ç•¥ä»¥è·å¾—æ›´å¤šç»“æœã€‚")
    print("=" * 60)

if __name__ == "__main__":
    main()
