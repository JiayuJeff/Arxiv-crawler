[
  {
    "arxiv_id": "2507.21836v1",
    "title": "AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement\n  Learning",
    "abstract": "Large Language Models (LLMs), when enhanced through reasoning-oriented\npost-training, evolve into powerful Large Reasoning Models (LRMs).\nTool-Integrated Reasoning (TIR) further extends their capabilities by\nincorporating external tools, but existing methods often rely on rigid,\npredefined tool-use patterns that risk degrading core language competence.\nInspired by the human ability to adaptively select tools, we introduce AutoTIR,\na reinforcement learning framework that enables LLMs to autonomously decide\nwhether and which tool to invoke during the reasoning process, rather than\nfollowing static tool-use strategies. AutoTIR leverages a hybrid reward\nmechanism that jointly optimizes for task-specific answer correctness,\nstructured output adherence, and penalization of incorrect tool usage, thereby\nencouraging both precise reasoning and efficient tool integration. Extensive\nevaluations across diverse knowledge-intensive, mathematical, and general\nlanguage modeling tasks demonstrate that AutoTIR achieves superior overall\nperformance, significantly outperforming baselines and exhibits superior\ngeneralization in tool-use behavior. These results highlight the promise of\nreinforcement learning in building truly generalizable and scalable TIR\ncapabilities in LLMs. The code and data are available at\nhttps://github.com/weiyifan1023/AutoTIR.",
    "authors": [
      "Yifan Wei",
      "Xiaoyan Yu",
      "Yixuan Weng",
      "Tengfei Pan",
      "Angsheng Li",
      "Li Du"
    ],
    "published": "2025-07-29T14:12:28Z",
    "updated": "2025-07-29T14:12:28Z",
    "categories": [
      "cs.CL"
    ],
    "page_url": "http://arxiv.org/abs/2507.21836v1",
    "pdf_url": "http://arxiv.org/pdf/2507.21836v1",
    "abstract_cn": "\n\n通过推理导向的后训练，大型语言模型（LLMs）可演变为强大的大型推理模型（LRMs）。工具集成推理（TIR）通过引入外部工具进一步扩展其能力，但现有方法通常依赖于刚性预定义的工具使用模式，这可能损害核心语言能力。受人类自适应工具选择能力的启发，我们提出AutoTIR，一种强化学习框架，使LLMs能够在推理过程中自主决定是否以及选择哪个工具调用，而非遵循静态工具使用策略。AutoTIR采用混合奖励机制，联合优化任务特定答案的正确性、结构化输出的遵循度以及对错误工具使用的惩罚，从而同时促进精确推理和高效工具集成。在多样化知识密集型、数学和通用语言建模任务中的广泛评估表明，AutoTIR实现了卓越的整体性能，显著优于基线模型，并在工具使用行为上表现出更强的泛化能力。这些结果凸显了强化学习在构建真正可泛化和可扩展的TIR能力方面的潜力。代码和数据已发布于https://github.com/weiyifan1023/AutoTIR。"
  },
  {
    "arxiv_id": "2507.19849v1",
    "title": "Agentic Reinforced Policy Optimization",
    "abstract": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO",
    "authors": [
      "Guanting Dong",
      "Hangyu Mao",
      "Kai Ma",
      "Licheng Bao",
      "Yifei Chen",
      "Zhongyuan Wang",
      "Zhongxia Chen",
      "Jiazhen Du",
      "Huiyang Wang",
      "Fuzheng Zhang",
      "Guorui Zhou",
      "Yutao Zhu",
      "Ji-Rong Wen",
      "Zhicheng Dou"
    ],
    "published": "2025-07-26T07:53:11Z",
    "updated": "2025-07-26T07:53:11Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "page_url": "http://arxiv.org/abs/2507.19849v1",
    "pdf_url": "http://arxiv.org/pdf/2507.19849v1",
    "abstract_cn": "\n\n大规模强化学习与可验证奖励（RLVR）在单轮推理任务中展示了其利用大语言模型（LLMs）潜力的有效性。在现实推理场景中，LLMs通常能够借助外部工具辅助任务求解过程。然而，当前的强化学习算法未能充分平衡模型内在的长时序推理能力与其在多轮工具交互中的熟练度。为弥合这一差距，我们提出了代理强化策略优化（ARPO），一种专为训练多轮LLM代理而设计的新型代理强化学习算法。通过初步实验，我们观察到LLMs在与外部工具交互后往往表现出高度不确定性行为，其特征是生成token的熵分布显著增加。受此观察启发，ARPO引入了基于熵的自适应展开机制，动态平衡全局轨迹采样与步级采样，从而在工具使用后高不确定性步骤中促进探索。通过整合优势归因估计，ARPO使LLMs能够内化工具使用交互中的优势差异。我们在计算推理、知识推理和深度搜索领域的13个具有挑战性的基准测试中进行了实验，结果表明ARPO优于轨迹级强化学习算法。值得注意的是，ARPO仅需现有方法一半的工具使用预算即可实现性能提升，为LLM代理与实时动态环境的对齐提供了可扩展解决方案。我们的代码和数据集已发布在https://github.com/dongguanting/ARPO"
  },
  {
    "arxiv_id": "2507.18884v1",
    "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service",
    "abstract": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems.",
    "authors": [
      "Ming Gong",
      "Xucheng Huang",
      "Ziheng Xu",
      "Vijayan K. Asari"
    ],
    "published": "2025-07-25T02:01:55Z",
    "updated": "2025-07-25T02:01:55Z",
    "categories": [
      "cs.CL"
    ],
    "page_url": "http://arxiv.org/abs/2507.18884v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18884v1",
    "abstract_cn": "\n\n高质量的对话对于电子商务客户服务至关重要，但传统的基于意图的系统在处理动态、多轮次交互时存在困难。我们提出了MindFlow+，一种自我演化的对话代理，通过将大型语言模型（LLMs）与模仿学习和离线强化学习（RL）相结合，学习特定领域的行为。MindFlow+引入了两种以数据为中心的机制来引导学习：工具增强的演示构建，通过知识增强和代理式（ReAct风格）交互向模型展示有效的工具使用；以及奖励条件数据建模，利用奖励信号将响应与任务特定目标对齐。为了评估模型在响应生成中的作用，我们引入了AI贡献率（AI Contribution Ratio），这一新指标量化了AI在对话中的参与程度。在真实电子商务对话上的实验表明，MindFlow+在上下文相关性、灵活性和任务准确性方面优于强大的基线模型。这些结果展示了将LLMs工具推理与奖励引导学习相结合，在构建领域专用、上下文感知的对话系统方面的潜力。"
  },
  {
    "arxiv_id": "2507.17275v2",
    "title": "Prolonging Tool Life: Learning Skillful Use of General-purpose Tools\n  through Lifespan-guided Reinforcement Learning",
    "abstract": "In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.",
    "authors": [
      "Po-Yen Wu",
      "Cheng-Yu Kuo",
      "Yuki Kadokawa",
      "Takamitsu Matsubara"
    ],
    "published": "2025-07-23T07:25:04Z",
    "updated": "2025-07-25T12:48:57Z",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "page_url": "http://arxiv.org/abs/2507.17275v2",
    "pdf_url": "http://arxiv.org/pdf/2507.17275v2",
    "abstract_cn": "\n\n在不可达环境中的不确定任务需求下，机器人通常依赖缺乏预定义使用策略的通用工具。这些工具未针对特定操作进行定制，其使用寿命对使用方式高度敏感。这带来了根本性挑战：机器人如何学习一种既完成任务又延长工具寿命的工具使用策略？本文通过引入一种强化学习（RL）框架来解决这一挑战，该框架在策略优化过程中将工具寿命作为关键因素。我们的框架利用有限元分析（FEA）和Miner准则，基于累积应力估计剩余使用寿命（RUL），并将RUL整合到RL奖励函数中，引导策略学习向寿命导向行为发展。为应对RUL仅能在任务执行后才能估算的问题，我们引入了自适应奖励归一化（ARN）机制，该机制根据估算的RUL动态调整奖励缩放，确保学习信号的稳定性。我们在模拟和真实场景的工具使用任务（包括使用多种通用工具的物体移动和开门任务）中验证了该方法。实验表明，所学策略显著延长工具寿命（模拟中最高达8.01倍），并能有效迁移到真实环境，展示了学习寿命引导型工具使用策略的实用价值。"
  },
  {
    "arxiv_id": "2507.13447v1",
    "title": "Theory-informed neural networks for particle physics",
    "abstract": "We present a theory-informed reinforcement-learning framework that recasts\nthe combinatorial assignment of final-state particles in hadron collider events\nas a Markov decision process. A transformer-based Deep Q-Network, rewarded at\neach step by the logarithmic change in the tree-level matrix element, learns to\nmap final-state particles to partons. Because the reward derives solely from\nfirst-principles theory, the resulting policy is label-free and fully\ninterpretable, allowing every reconstructed particle to be traced to a definite\npartonic origin. The method is validated on event reconstruction for\n$t\\bar{t}$, $t\\bar{t}W$, and $t\\bar{t}t\\bar{t}$ processes at the Large Hadron\nCollider. The method maintains robust performance across all processes,\ndemonstrating its scaling with increasing combinatorial complexity. We\ndemonstrate how this method can be used to build a theory-informed classifier\nfor effective discrimination of longitudinal $W^{+}W^{-}$ pairs, and show that\nwe can construct theory-informed anomaly-detection tools using background\nprocess matrix elements. Building on theoretical calculations, this method\noffers a transparent alternative to black-box classifiers. Being built on the\nmatrix element, the classification and anomaly scores naturally respect all\nphysical symmetries and are much less susceptible to the implicit biases common\nto other methods. Thus, it provides a framework for precision measurements,\nhypothesis testing, and anomaly searches at the High-Luminosity LHC.",
    "authors": [
      "Barry M. Dillon",
      "Michael Spannowsky"
    ],
    "published": "2025-07-17T18:00:05Z",
    "updated": "2025-07-17T18:00:05Z",
    "categories": [
      "hep-ph",
      "hep-ex",
      "hep-th"
    ],
    "page_url": "http://arxiv.org/abs/2507.13447v1",
    "pdf_url": "http://arxiv.org/pdf/2507.13447v1",
    "abstract_cn": "\n\n我们提出了一种基于理论的强化学习框架，将强子对撞机事件中末态粒子的组合分配问题重新表述为马尔可夫决策过程。基于Transformer的深度Q网络通过树级矩阵元的对数变化作为每一步的奖励，学习将末态粒子映射到部分子。由于奖励完全来源于第一性原理理论，所得到的策略是无标签且完全可解释的，允许每个重建粒子追溯到确定的部分子起源。该方法在大型强子对撞机的t̄t、t̄tW和t̄tt̄t过程事件重建中得到验证，在所有过程中均保持稳健性能，展示了其对组合复杂度增加的扩展能力。我们演示了如何利用该方法构建理论驱动的分类器以实现纵向W⁺W⁻对的有效区分，并展示了如何使用背景过程矩阵元构建理论驱动的异常检测工具。基于理论计算，该方法为黑箱分类器提供了透明的替代方案。由于其基于矩阵元，分类和异常分数自然尊重所有物理对称性，且比其他方法更少受到隐式偏差的影响。因此，它为高亮度大型强子对撞机的精确测量、假设检验和异常搜索提供了框架。"
  },
  {
    "arxiv_id": "2507.08270v1",
    "title": "Agent Safety Alignment via Reinforcement Learning",
    "abstract": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents.",
    "authors": [
      "Zeyang Sha",
      "Hanling Tian",
      "Zhuoer Xu",
      "Shiwen Cui",
      "Changhua Meng",
      "Weiqiang Wang"
    ],
    "published": "2025-07-11T02:34:16Z",
    "updated": "2025-07-11T02:34:16Z",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "page_url": "http://arxiv.org/abs/2507.08270v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08270v1",
    "abstract_cn": "\n\n自主大型语言模型（LLM）代理具备工具使用能力的出现，带来了超越传统对话滥用的新安全风险。这些被授权执行外部功能的代理，既面临用户发起的威胁（如对抗性提示），也面临工具发起的威胁（如被入侵工具的恶意输出）。本文提出了首个面向工具使用代理的统一安全对齐框架，通过结构化推理和沙箱强化学习实现对双重威胁通道的处理。我们引入了三模态分类法，将用户提示和工具响应分别划分为良性、恶意和敏感三类，并定义了基于策略的决策模型。该框架采用定制设计的沙箱环境模拟真实工具执行，支持细粒度奖励塑造。通过在公开和自建基准（包括Agent SafetyBench、InjecAgent和BFCL）上的广泛评估，我们证明安全对齐代理在保持良性任务强效用的同时，显著提升了对安全威胁的抵御能力。实验结果表明安全性与有效性可以协同优化，为自主LLM代理的可信部署奠定了基础。"
  },
  {
    "arxiv_id": "2507.00951v3",
    "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
    "abstract": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.",
    "authors": [
      "Rizwan Qureshi",
      "Ranjan Sapkota",
      "Abbas Shah",
      "Amgad Muneer",
      "Anas Zafar",
      "Ashmal Vayani",
      "Maged Shoman",
      "Abdelrahman B. M. Eldaly",
      "Kai Zhang",
      "Ferhat Sadak",
      "Shaina Raza",
      "Xinqi Fan",
      "Ravid Shwartz-Ziv",
      "Hong Yan",
      "Vinjia Jain",
      "Aman Chadha",
      "Manoj Karkee",
      "Jia Wu",
      "Seyedali Mirjalili"
    ],
    "published": "2025-07-01T16:52:25Z",
    "updated": "2025-07-12T02:50:17Z",
    "categories": [
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2507.00951v3",
    "pdf_url": "http://arxiv.org/pdf/2507.00951v3",
    "abstract_cn": "\n\n机器能否真正像人类一样思考、推理并行动？这一持续存在的问题仍在塑造人工通用智能（AGI）的探索路径。尽管GPT-4.5、DeepSeek、Claude 3.5 Sonnet、Phi-4和Grok 3等模型展现出多模态流畅性和部分推理能力，但这些系统本质上仍受限于基于token级别的预测机制和缺乏具身化能动性。本文对AGI发展进行了跨学科综述，涵盖人工智能、认知神经科学、心理学、生成模型和智能体系统等领域。我们分析了通用智能的架构与认知基础，重点强调模块化推理、持久记忆和多智能体协作的作用。特别指出，Agentic RAG框架通过整合检索、规划和动态工具使用，正在推动更适应性的行为模式。我们讨论了泛化策略，包括信息压缩、测试时适应和无训练方法，这些被视作实现灵活、跨领域智能的关键路径。视觉-语言模型（VLMs）不再仅作为感知模块，而是演变为具身化理解和协作任务完成的交互界面。本文进一步论证，真正的智能并非源于规模本身，而是记忆与推理的整合：通过模块化、交互式和自优化组件的协同，压缩机制实现适应性行为。结合神经符号系统、强化学习和认知脚手架的最新进展，我们探讨了当前架构如何弥合统计学习与目标导向认知之间的鸿沟。最后，我们识别出通向AGI道路上的关键科学、技术和伦理挑战。"
  },
  {
    "arxiv_id": "2506.13666v1",
    "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems",
    "abstract": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git.",
    "authors": [
      "Junfeng Fang",
      "Zijun Yao",
      "Ruipeng Wang",
      "Haokai Ma",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "published": "2025-06-16T16:24:31Z",
    "updated": "2025-06-16T16:24:31Z",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2506.13666v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13666v1",
    "abstract_cn": "\n\n大型语言模型（LLM）的发展已进入经验驱动时代，其标志是通过强化学习和工具使用代理实现的环境反馈驱动学习。这催生了模型上下文协议（MCP）的出现，该协议定义了LLM与外部服务（如API和数据）交互的标准。然而，随着MCP成为LLM代理系统的事实标准，也引入了新的安全风险。特别是，MCP将不受LLM开发者控制的第三方服务引入代理系统。这些第三方MCP服务提供商可能具有恶意动机，并有经济诱因而利用漏洞破坏用户-代理交互。在本立场论文中，我们呼吁LLM安全研究社区重点关注MCP引入的新安全风险问题，并开发新技术构建安全的MCP驱动代理系统。为阐明我们的立场，论文从三个关键部分展开论述：（1）我们首先构建了\\framework框架，这是一个可控框架用于检验MCP驱动代理系统的安全问题；（2）随后通过一系列试点实验验证MCP驱动代理系统的安全风险是真实存在的威胁，且其防御并非易事；（3）最后通过展示构建安全MCP驱动代理系统的路线图提出未来展望。具体而言，我们呼吁研究者重点关注以下研究方向：红队测试、MCP安全LLM开发、MCP安全性评估、MCP安全数据积累、MCP服务防护以及MCP安全生态系统构建。我们希望本立场论文能提升研究社区对MCP安全性的关注，并鼓励更多研究者加入这一重要研究方向。代码已开源至https://github.com/littlelittlenine/SafeMCP.git。"
  },
  {
    "arxiv_id": "2506.07900v1",
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "abstract": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
    "authors": [
      " MiniCPM Team",
      "Chaojun Xiao",
      "Yuxuan Li",
      "Xu Han",
      "Yuzhuo Bai",
      "Jie Cai",
      "Haotian Chen",
      "Wentong Chen",
      "Xin Cong",
      "Ganqu Cui",
      "Ning Ding",
      "Shengdan Fan",
      "Yewei Fang",
      "Zixuan Fu",
      "Wenyu Guan",
      "Yitong Guan",
      "Junshao Guo",
      "Yufeng Han",
      "Bingxiang He",
      "Yuxiang Huang",
      "Cunliang Kong",
      "Qiuzuo Li",
      "Siyuan Li",
      "Wenhao Li",
      "Yanghao Li",
      "Yishan Li",
      "Zhen Li",
      "Dan Liu",
      "Biyuan Lin",
      "Yankai Lin",
      "Xiang Long",
      "Quanyu Lu",
      "Yaxi Lu",
      "Peiyan Luo",
      "Hongya Lyu",
      "Litu Ou",
      "Yinxu Pan",
      "Zekai Qu",
      "Qundong Shi",
      "Zijun Song",
      "Jiayuan Su",
      "Zhou Su",
      "Ao Sun",
      "Xianghui Sun",
      "Peijun Tang",
      "Fangzheng Wang",
      "Feng Wang",
      "Shuo Wang",
      "Yudong Wang",
      "Yesai Wu",
      "Zhenyu Xiao",
      "Jie Xie",
      "Zihao Xie",
      "Yukun Yan",
      "Jiarui Yuan",
      "Kaihuo Zhang",
      "Lei Zhang",
      "Linyue Zhang",
      "Xueren Zhang",
      "Yudi Zhang",
      "Hengyu Zhao",
      "Weilin Zhao",
      "Weilun Zhao",
      "Yuanqian Zhao",
      "Zhi Zheng",
      "Ge Zhou",
      "Jie Zhou",
      "Wei Zhou",
      "Zihan Zhou",
      "Zixuan Zhou",
      "Zhiyuan Liu",
      "Guoyang Zeng",
      "Chao Jia",
      "Dahai Li",
      "Maosong Sun"
    ],
    "published": "2025-06-09T16:16:50Z",
    "updated": "2025-06-09T16:16:50Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2506.07900v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07900v1",
    "abstract_cn": "\n\n本文介绍了MiniCPM4，这是一种专为端侧设备设计的高效大型语言模型（LLM）。我们通过在四个关键维度上的系统性创新实现了这一效率：模型架构、训练数据、训练算法和推理系统。具体而言，在模型架构方面，我们提出了InfLLM v2，这是一种可训练的稀疏注意力机制，能够加速长上下文处理中的预填充和解码阶段。在训练数据方面，我们提出了UltraClean，一种高效且准确的预训练数据过滤和生成策略，以及UltraChat v2，一个全面的监督微调数据集。这些数据集仅需8万亿训练token即可实现令人满意的模型性能。在训练算法方面，我们提出了ModelTunnel v2用于高效的预训练策略搜索，并通过引入分块式展开改进了现有的后训练方法，实现负载均衡的强化学习和数据高效的三元LLM（BitCPM）。在推理系统方面，我们提出了CPM.cu，该系统整合了稀疏注意力、模型量化和推测采样，实现了高效的预填充和解码。为满足多样化的端侧需求，MiniCPM4提供两个版本，参数量分别为0.5B和8B。充分的评估结果表明，MiniCPM4在多个基准测试中优于同规模的开源模型，凸显了其效率和效果。值得注意的是，MiniCPM4-8B在处理长序列时相比Qwen3-8B展现出显著的速度提升。通过进一步适配，MiniCPM4成功支持了多种应用，包括可信调查生成和基于模型上下文协议的工具使用，充分展示了其广泛的应用性。"
  },
  {
    "arxiv_id": "2506.07829v1",
    "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal\n  Information",
    "abstract": "Reinforcement learning (RL) algorithms can find an optimal policy for a\nsingle agent to accomplish a particular task. However, many real-world problems\nrequire multiple agents to collaborate in order to achieve a common goal. For\nexample, a robot executing a task in a warehouse may require the assistance of\na drone to retrieve items from high shelves. In Decentralized Multi-Agent RL\n(DMARL), agents learn independently and then combine their policies at\nexecution time, but often must satisfy constraints on compatibility of local\npolicies to ensure that they can achieve the global task when combined. In this\npaper, we study how providing high-level symbolic knowledge to agents can help\naddress unique challenges of this setting, such as privacy constraints,\ncommunication limitations, and performance concerns. In particular, we extend\nthe formal tools used to check the compatibility of local policies with the\nteam task, making decentralized training with theoretical guarantees usable in\nmore scenarios. Furthermore, we empirically demonstrate that symbolic knowledge\nabout the temporal evolution of events in the environment can significantly\nexpedite the learning process in DMARL.",
    "authors": [
      "Jan Corazza",
      "Hadi Partovi Aria",
      "Hyohun Kim",
      "Daniel Neider",
      "Zhe Xu"
    ],
    "published": "2025-06-09T14:53:03Z",
    "updated": "2025-06-09T14:53:03Z",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2506.07829v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07829v1",
    "abstract_cn": "\n\n强化学习（RL）算法可以为单个智能体找到最优策略以完成特定任务。然而，许多现实世界的问题需要多个智能体协作才能实现共同目标。例如，仓库中执行任务的机器人可能需要无人机协助取下高货架上的物品。在去中心化多智能体强化学习（DMARL）中，智能体独立学习并在执行时组合其策略，但通常必须满足局部策略兼容性的约束以确保组合后能实现全局任务。本文研究了向智能体提供高层符号知识如何帮助应对该场景下的独特挑战，如隐私约束、通信限制和性能问题。具体而言，我们扩展了用于检查局部策略与团队任务兼容性的形式化工具，使具有理论保证的去中心化训练能够应用于更多场景。此外，实证表明关于环境事件时间演化的符号知识可以显著加速DMARL中的学习过程。"
  },
  {
    "arxiv_id": "2506.01716v1",
    "title": "Self-Challenging Language Model Agents",
    "abstract": "Large language models are quickly becoming the foundation for intelligent\nagents that are capable of using tools. However, training such agents is\nchallenging because it requires human creation and annotation of a diverse set\nof tasks, tools, and evaluation criteria. In this paper, we propose the\nSelf-Challenging framework for training an agent on high-quality tasks that are\ngenerated by itself. The agent first plays the role of challenger and generates\na task after interacting with the given tools. The tasks take the form of a\nnovel general class of problems termed Code-as-Task, which are defined by an\ninstruction, a verification function and solution and failure cases which serve\nas tests, allowing to filter only for high-quality tasks. The agent then takes\nan executor role and trains on those tasks with reinforcement learning using\nthe evaluation feedback as a reward. Evaluation on two existing multi-turn\ntool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging\nframework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,\ndespite using only self-generated training data.",
    "authors": [
      "Yifei Zhou",
      "Sergey Levine",
      "Jason Weston",
      "Xian Li",
      "Sainbayar Sukhbaatar"
    ],
    "published": "2025-06-02T14:23:33Z",
    "updated": "2025-06-02T14:23:33Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "page_url": "http://arxiv.org/abs/2506.01716v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01716v1",
    "abstract_cn": "\n\n大型语言模型正迅速成为能够使用工具的智能体的基础。然而，训练此类智能体面临挑战，因为这需要人工创建和标注多样化的任务集、工具集和评估标准。本文提出了一种自我挑战框架，用于训练由智能体自身生成的高质量任务。该智能体首先扮演挑战者角色，通过与给定工具交互生成任务。这些任务采用一种新型通用问题类形式，称为代码即任务（Code-as-Task），由指令、验证函数及作为测试的解决方案和失败案例构成，从而筛选出高质量任务。随后智能体切换为执行者角色，利用评估反馈作为奖励，通过强化学习在这些任务上进行训练。在现有两个多轮工具使用智能体基准M3ToolEval和TauBench上的评估表明，自我挑战框架在仅使用自生成训练数据的情况下，使Llama-3.1-8B-Instruct模型性能提升了两倍以上。"
  },
  {
    "arxiv_id": "2505.24480v1",
    "title": "Towards Effective Code-Integrated Reasoning",
    "abstract": "In this paper, we investigate code-integrated reasoning, where models\ngenerate code when necessary and integrate feedback by executing it through a\ncode interpreter. To acquire this capability, models must learn when and how to\nuse external code tools effectively, which is supported by tool-augmented\nreinforcement learning (RL) through interactive learning. Despite its benefits,\ntool-augmented RL can still suffer from potential instability in the learning\ndynamics. In light of this challenge, we present a systematic approach to\nimproving the training effectiveness and stability of tool-augmented RL for\ncode-integrated reasoning. Specifically, we develop enhanced training\nstrategies that balance exploration and stability, progressively building\ntool-use capabilities while improving reasoning performance. Through extensive\nexperiments on five mainstream mathematical reasoning benchmarks, our model\ndemonstrates significant performance improvements over multiple competitive\nbaselines. Furthermore, we conduct an in-depth analysis of the mechanism and\neffect of code-integrated reasoning, revealing several key insights, such as\nthe extension of model's capability boundaries and the simultaneous improvement\nof reasoning efficiency through code integration. All data and code for\nreproducing this work are available at: https://github.com/RUCAIBox/CIR.",
    "authors": [
      "Fei Bai",
      "Yingqian Min",
      "Beichen Zhang",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Lei Fang",
      "Zheng Liu",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-30T11:30:18Z",
    "updated": "2025-05-30T11:30:18Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "page_url": "http://arxiv.org/abs/2505.24480v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24480v1",
    "abstract_cn": "\n\n本文研究了代码集成推理，其中模型在必要时生成代码并通过代码解释器执行以集成反馈。为了获得这种能力，模型必须学习何时以及如何有效使用外部代码工具，这通过交互式学习的工具增强强化学习（RL）来实现。尽管具有诸多优势，工具增强的强化学习仍可能面临学习动态中的潜在不稳定性问题。针对这一挑战，我们提出了一种系统性方法，用于提升工具增强强化学习在代码集成推理中的训练效果和稳定性。具体而言，我们开发了平衡探索与稳定性的增强训练策略，在提升推理性能的同时逐步构建工具使用能力。通过在五个主流数学推理基准测试上的广泛实验，我们的模型在多个竞争性基线模型上表现出显著的性能提升。此外，我们对代码集成推理的机制与效果进行了深入分析，揭示了若干关键发现，例如模型能力边界的扩展以及通过代码集成实现的推理效率同步提升。所有用于复现本研究的数据和代码均可在以下地址获取：https://github.com/RUCAIBox/CIR。"
  },
  {
    "arxiv_id": "2505.19255v3",
    "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on\n  Multimodal Tool Use",
    "abstract": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools.",
    "authors": [
      "Mingyuan Wu",
      "Jingcheng Yang",
      "Jize Jiang",
      "Meitang Li",
      "Kaizhuo Yan",
      "Hanchao Yu",
      "Minjia Zhang",
      "Chengxiang Zhai",
      "Klara Nahrstedt"
    ],
    "published": "2025-05-25T18:23:39Z",
    "updated": "2025-06-11T21:47:49Z",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2505.19255v3",
    "pdf_url": "http://arxiv.org/pdf/2505.19255v3",
    "abstract_cn": "\n\n强化学习微调（RFT）通过实现长链推理、自我修正和有效工具使用，显著提升了大语言模型（LLMs）的推理能力。尽管近期研究尝试将RFT扩展至视觉语言模型（VLMs），但这些方法主要局限于纯文本推理，且依赖静态图像输入，未能实现真正的多模态响应推理。相比之下，测试时方法如Visual Sketchpad虽引入了视觉步骤，但缺乏训练机制。\n\n我们提出VTool-R1，首个通过交织文本和中间视觉推理步骤来训练VLM生成多模态推理链的框架。VTool-R1将基于Python的视觉编辑工具整合至RFT流程中，使VLM能够学习何时以及如何生成有助于最终推理的视觉推理步骤。通过与任务准确率挂钩的基于结果的奖励进行训练，我们的方法无需依赖过程监督即可激发推理中的战略性视觉工具使用。在图表和表格的结构化视觉问答实验中，VTool-R1通过教会VLM\"以图像思考\"并生成工具辅助的多模态推理链，显著提升了推理性能。"
  },
  {
    "arxiv_id": "2505.18098v1",
    "title": "Planning without Search: Refining Frontier LLMs with Offline\n  Goal-Conditioned RL",
    "abstract": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.",
    "authors": [
      "Joey Hong",
      "Anca Dragan",
      "Sergey Levine"
    ],
    "published": "2025-05-23T16:51:54Z",
    "updated": "2025-05-23T16:51:54Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2505.18098v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18098v1",
    "abstract_cn": "\n\n大型语言模型（LLMs）在问答和对话等任务中表现出色，但涉及互动的复杂任务（如谈判和说服）需要额外的长期推理和规划能力。强化学习（RL）微调原则上可以实现这种规划能力，但存在限制可扩展性的缺陷。具体而言，多轮强化学习训练会带来高内存和计算成本，这种成本在训练LLMs作为策略时尤为显著。此外，最大的LLMs并未提供通过此类方式训练所需的API接口。因此，当前改进LLMs推理能力的方法主要依赖复杂的提示机制而非强化学习微调。为解决这一问题，我们提出了一种新方法，通过目标条件价值函数引导LLM智能体的推理过程，该方法可扩展至基于API的大型模型。这些价值函数能够预测特定动作下任务的展开方式，使LLM智能体能够评估多种可能结果（包括正向和负向结果）以实现有效规划。此外，这些价值函数在推理步骤而非完整动作上进行训练，形成简洁轻量的模块，从而促进多轮交互中的决策过程。我们在需要互动的任务（包括工具使用、社会推断和对话）上验证了该方法，实验结果表明其在保持高效性和可扩展性的同时，性能优于强化学习微调和提示方法。"
  },
  {
    "arxiv_id": "2505.16410v1",
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
    "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.",
    "authors": [
      "Guanting Dong",
      "Yifei Chen",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Hongjin Qian",
      "Yutao Zhu",
      "Hangyu Mao",
      "Guorui Zhou",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-22T09:00:19Z",
    "updated": "2025-05-22T09:00:19Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "page_url": "http://arxiv.org/abs/2505.16410v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16410v1",
    "abstract_cn": "\n\n近年来，大型语言模型（LLMs）通过大规模强化学习（RL）展现出显著的推理能力。然而，利用RL算法实现LLMs的高效多工具协同推理仍是一个开放性挑战。本文提出Tool-Star，一种基于强化学习的框架，旨在使LLMs在逐步推理过程中自主调用多个外部工具。Tool-Star整合了六类工具，并在数据合成与训练中引入系统性设计。为解决工具使用数据稀缺问题，我们提出通用的工具集成推理数据合成流水线，通过工具集成提示与基于提示的采样相结合，实现工具使用轨迹的自动化和可扩展生成。随后的质量归一化与难度感知分类流程可过滤低质量样本，并按难度梯度组织数据集。此外，我们设计了两阶段训练框架以增强多工具协同推理能力：（1）冷启动微调阶段，通过工具调用反馈引导LLMs探索推理模式；（2）具有分层奖励设计的多工具自批评RL算法，强化奖励理解并促进有效工具协作。在10余个具有挑战性的推理基准上的实验分析验证了Tool-Star的有效性与高效性。代码已开源至https://github.com/dongguanting/Tool-Star。"
  },
  {
    "arxiv_id": "2505.16282v1",
    "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience\n  Replay",
    "abstract": "Training large language models (LLMs) as interactive agents for controlling\ngraphical user interfaces (GUIs) presents a unique challenge to optimize\nlong-horizon action sequences with multimodal feedback from complex\nenvironments. While recent works have advanced multi-turn reinforcement\nlearning (RL) for reasoning and tool-using capabilities in LLMs, their\napplication to GUI-based agents remains relatively underexplored due to the\ndifficulty of sparse rewards, delayed feedback, and high rollout costs. In this\npaper, we investigate end-to-end policy optimization for vision-language-based\nGUI agents with the aim of improving performance on complex, long-horizon\ncomputer tasks. We propose Agentic Replay Policy Optimization (ARPO), an\nend-to-end RL approach that augments Group Relative Policy Optimization (GRPO)\nwith a replay buffer to reuse the successful experience across training\niterations. To further stabilize the training process, we propose a task\nselection strategy that filters tasks based on baseline agent performance,\nallowing the agent to focus on learning from informative interactions.\nAdditionally, we compare ARPO with offline preference optimization approaches,\nhighlighting the advantages of policy-based methods in GUI environments.\nExperiments on the OSWorld benchmark demonstrate that ARPO achieves competitive\nresults, establishing a new performance baseline for LLM-based GUI agents\ntrained via reinforcement learning. Our findings underscore the effectiveness\nof reinforcement learning for training multi-turn, vision-language GUI agents\ncapable of managing complex real-world UI interactions. Codes and\nmodels:https://github.com/dvlab-research/ARPO.git.",
    "authors": [
      "Fanbin Lu",
      "Zhisheng Zhong",
      "Shu Liu",
      "Chi-Wing Fu",
      "Jiaya Jia"
    ],
    "published": "2025-05-22T06:24:32Z",
    "updated": "2025-05-22T06:24:32Z",
    "categories": [
      "cs.CV"
    ],
    "page_url": "http://arxiv.org/abs/2505.16282v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16282v1",
    "abstract_cn": "\n\n将大型语言模型（LLMs）训练为交互代理以控制图形用户界面（GUIs）面临独特挑战，需优化复杂环境中多模态反馈下的长时序动作序列。尽管近期研究在LLMs的多轮强化学习（RL）推理与工具使用能力方面取得进展，但其在GUI代理中的应用仍因稀疏奖励、延迟反馈和高采样成本而相对未被充分探索。本文研究基于视觉语言的GUI代理端到端策略优化方法，旨在提升复杂长时序计算机任务的性能。我们提出代理回放策略优化（ARPO），这是一种通过引入回放缓冲区增强组相对策略优化（GRPO）的端到端RL方法，可在训练迭代中复用成功经验。为稳定训练过程，我们设计了基于基线代理性能的任务选择策略，使代理专注于具有信息量的交互学习。此外，我们对比了ARPO与离线偏好优化方法，突显策略类方法在GUI环境中的优势。OSWorld基准实验表明，ARPO取得具有竞争力的成果，为基于强化学习训练的LLM GUI代理建立新性能基准。研究结果验证了强化学习在训练多轮视觉语言GUI代理以处理复杂真实界面交互方面的有效性。代码与模型：https://github.com/dvlab-research/ARPO.git"
  },
  {
    "arxiv_id": "2505.14362v2",
    "title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement\n  Learning",
    "abstract": "Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes.",
    "authors": [
      "Ziwei Zheng",
      "Michael Yang",
      "Jack Hong",
      "Chenxiao Zhao",
      "Guohai Xu",
      "Le Yang",
      "Chao Shen",
      "Xing Yu"
    ],
    "published": "2025-05-20T13:48:11Z",
    "updated": "2025-05-26T13:19:11Z",
    "categories": [
      "cs.CV"
    ],
    "page_url": "http://arxiv.org/abs/2505.14362v2",
    "pdf_url": "http://arxiv.org/pdf/2505.14362v2",
    "abstract_cn": "\n\n大型视觉-语言模型（VLMs）在多模态理解和推理方面展现出强大能力，但其推理过程主要受限于基于文本的范式。实现与人类认知过程相似的视觉与文本推理无缝整合仍是一个重大挑战，特别是如何将先进的视觉输入处理有效融入推理机制仍是一个开放性问题。为此，本文探索了交错式多模态推理范式，提出DeepEyes模型——该模型通过端到端强化学习（无需冷启动监督微调）激励\"以图像进行思考\"的能力。值得注意的是，这种能力源于模型自身固有的基础能力，作为工具被激活而非依赖独立专用模型。具体而言，我们设计了以工具使用为导向的数据选择机制和奖励策略，以促进成功的工具辅助推理路径。DeepEyes在细粒度感知与推理基准测试中取得显著性能提升，同时在基础能力、幻觉控制和数学推理任务中均表现出改进。有趣的是，我们观察到工具调用行为从初始探索到高效准确利用的演变过程，以及与人类视觉推理高度相似的多样化思维模式。代码已发布于https://github.com/Visual-Agent/DeepEyes。"
  },
  {
    "arxiv_id": "2505.11821v1",
    "title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit\n  Assignment",
    "abstract": "This paper investigates approaches to enhance the reasoning capabilities of\nLarge Language Model (LLM) agents using Reinforcement Learning (RL).\nSpecifically, we focus on multi-turn tool-use scenarios, which can be naturally\nmodeled as Markov Decision Processes (MDPs). While existing approaches often\ntrain multi-turn LLM agents with trajectory-level advantage estimation in\nbandit settings, they struggle with turn-level credit assignment across\nmultiple decision steps, limiting their performance on multi-turn reasoning\ntasks. To address this, we introduce a fine-grained turn-level advantage\nestimation strategy to enable more precise credit assignment in multi-turn\nagent interactions. The strategy is general and can be incorporated into\nvarious RL algorithms such as Group Relative Preference Optimization (GRPO).\nOur experimental evaluation on multi-turn reasoning and search-based tool-use\ntasks with GRPO implementations highlights the effectiveness of the MDP\nframework and the turn-level credit assignment in advancing the multi-turn\nreasoning capabilities of LLM agents in complex decision-making settings. Our\nmethod achieves 100% success in tool execution and 50% accuracy in exact answer\nmatching, significantly outperforming baselines, which fail to invoke tools and\nachieve only 20-30% exact match accuracy.",
    "authors": [
      "Siliang Zeng",
      "Quan Wei",
      "William Brown",
      "Oana Frunza",
      "Yuriy Nevmyvaka",
      "Mingyi Hong"
    ],
    "published": "2025-05-17T04:09:46Z",
    "updated": "2025-05-17T04:09:46Z",
    "categories": [
      "cs.LG"
    ],
    "page_url": "http://arxiv.org/abs/2505.11821v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11821v1",
    "abstract_cn": "\n\n本文研究了利用强化学习（Reinforcement Learning, RL）增强大型语言模型（Large Language Model, LLM）代理推理能力的方法。具体而言，我们聚焦于可自然建模为马尔可夫决策过程（Markov Decision Process, MDP）的多轮工具使用场景。尽管现有方法通常在bandit设置中通过轨迹级优势估计训练多轮LLM代理，但它们在多步决策中难以实现回合级的信用分配，限制了其在多轮推理任务中的性能。为此，我们引入了一种细粒度的回合级优势估计策略，以提升多轮代理交互中的信用分配精度。该策略具有通用性，可集成到Group Relative Preference Optimization（GRPO）等多种RL算法中。基于GRPO实现的多轮推理与搜索型工具使用任务的实验评估表明，MDP框架与回合级信用分配机制能有效提升LLM代理在复杂决策场景中的多轮推理能力。我们的方法在工具执行成功率和精确答案匹配准确率上分别达到100%和50%，显著优于基线方法（基线方法无法调用工具，精确匹配准确率仅为20-30%）。"
  },
  {
    "arxiv_id": "2505.07773v3",
    "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
    "abstract": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
    "authors": [
      "Xinji Mai",
      "Haotian Xu",
      "Xing W",
      "Weinong Wang",
      "Jian Hu",
      "Yingying Zhang",
      "Wenqiang Zhang"
    ],
    "published": "2025-05-12T17:23:34Z",
    "updated": "2025-07-23T14:15:28Z",
    "categories": [
      "cs.AI"
    ],
    "page_url": "http://arxiv.org/abs/2505.07773v3",
    "pdf_url": "http://arxiv.org/pdf/2505.07773v3",
    "abstract_cn": "\n\n大型语言模型（LLMs）在需要精确可验证计算的数学推理任务中往往表现不佳。尽管基于结果奖励的强化学习（RL）能够提升文本推理能力，但理解智能体如何自主学习利用代码执行等外部工具仍具有重要意义。我们研究了基于结果奖励的强化学习方法ZeroTIR（工具集成推理），通过训练基础LLM在没有监督工具使用示例的情况下自发生成并执行Python代码解决数学问题。我们的核心贡献在于证明随着强化学习训练的推进，关键指标呈现出可预测的扩展性。具体而言，我们观察到显著的正相关关系：训练步数增加会带来自发代码执行频率的提升、平均响应长度的增加以及最终任务准确率的提高。这表明训练中投入的计算量与有效工具增强推理策略的出现之间存在可量化的关联。我们构建了一个包含解耦代码执行环境的稳健框架，并在标准强化学习算法和框架中验证了研究发现。实验表明，ZeroTIR在具有挑战性的数学基准测试中显著超越了非工具ZeroRL基线模型。我们的研究为自主工具使用在智能体强化学习中的获取与扩展提供了基础性理解，为未来研究提供了可复现的基准。代码已发布在https://github.com/yyht/openrlhf_async_pipline。"
  },
  {
    "arxiv_id": "2505.01115v1",
    "title": "Exploring Equity of Climate Policies using Multi-Agent Multi-Objective\n  Reinforcement Learning",
    "abstract": "Addressing climate change requires coordinated policy efforts of nations\nworldwide. These efforts are informed by scientific reports, which rely in part\non Integrated Assessment Models (IAMs), prominent tools used to assess the\neconomic impacts of climate policies. However, traditional IAMs optimize\npolicies based on a single objective, limiting their ability to capture the\ntrade-offs among economic growth, temperature goals, and climate justice. As a\nresult, policy recommendations have been criticized for perpetuating\ninequalities, fueling disagreements during policy negotiations. We introduce\nJustice, the first framework integrating IAM with Multi-Objective Multi-Agent\nReinforcement Learning (MOMARL). By incorporating multiple objectives, Justice\ngenerates policy recommendations that shed light on equity while balancing\nclimate and economic goals. Further, using multiple agents can provide a\nrealistic representation of the interactions among the diverse policy actors.\nWe identify equitable Pareto-optimal policies using our framework, which\nfacilitates deliberative decision-making by presenting policymakers with the\ninherent trade-offs in climate and economic policy.",
    "authors": [
      "Palok Biswas",
      "Zuzanna Osika",
      "Isidoro Tamassia",
      "Adit Whorra",
      "Jazmin Zatarain-Salazar",
      "Jan Kwakkel",
      "Frans A. Oliehoek",
      "Pradeep K. Murukannaiah"
    ],
    "published": "2025-05-02T08:52:56Z",
    "updated": "2025-05-02T08:52:56Z",
    "categories": [
      "cs.LG"
    ],
    "page_url": "http://arxiv.org/abs/2505.01115v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01115v1",
    "abstract_cn": "\n\n应对气候变化需要全球各国协调一致的政策努力。这些努力以科学报告为依据，而科学报告部分依赖于综合评估模型（IAMs）——这一用于评估气候政策经济影响的重要工具。然而，传统IAMs基于单一目标优化政策，限制了其捕捉经济增长、温控目标与气候正义之间权衡的能力。因此，政策建议常被批评为加剧不平等，导致政策谈判中的分歧。我们引入Justice框架，这是首个将IAM与多目标多智能体强化学习（MOMARL）相结合的框架。通过整合多重目标，Justice生成的政策建议在平衡气候与经济目标的同时揭示公平性。此外，采用多智能体方法可更真实地表征多元政策主体间的互动关系。我们通过该框架识别出公平的帕累托最优政策，通过呈现气候与经济政策中固有的权衡关系，为政策制定者提供更具思辨性的决策支持。"
  }
]